{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict name based on first several letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "print(f\"{torch.cuda.is_available()=}\")\n",
    "device=\"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device=\"cuda:0\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "itos = {0:'.'}\n",
    "for i, c in enumerate(string.ascii_lowercase):\n",
    "    itos[i+1]=c  \n",
    "stoi = {s:i for i, s in itos.items()}\n",
    "print(stoi)\n",
    "voc_size=len(itos)\n",
    "print(f\"{voc_size=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(ss):\n",
    "    res = [stoi[c] for c in ss]\n",
    "    return res\n",
    "\n",
    "def decode(ii, tilldot=False):\n",
    "    ch = False\n",
    "    res = []\n",
    "    for i in ii:\n",
    "        if i == 0 and tilldot and ch:\n",
    "            break\n",
    "        if not(ch) and i != 0:\n",
    "            ch = True\n",
    "        res.append(itos[i])\n",
    "    return ''.join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_f = \"names.txt\"\n",
    "with open(names_f) as f:\n",
    "    words = f.read().splitlines()\n",
    "\n",
    "#random.seed(42)\n",
    "random.shuffle(words)\n",
    "print(words[:3])\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_word(w, bsz, X, Y):\n",
    "    x = \".\"*bsz\n",
    "    xi = [0]*bsz\n",
    "    for y in w:\n",
    "        yi = stoi[y]\n",
    "        X.append(xi)\n",
    "        Y.append(yi)\n",
    "        xi = xi[1:]\n",
    "        xi.append(yi)\n",
    "    X.append(xi)\n",
    "    Y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att=3\n",
    "emb=2\n",
    "hidden = 100\n",
    "\n",
    "Xa, Ya = [], []\n",
    "for w in words:\n",
    "    add_word(w, att, Xa, Ya)\n",
    "X = torch.tensor(Xa, device=device)\n",
    "Y = torch.tensor(Ya, device=device)\n",
    "print(f\"{X.shape=}\")\n",
    "print(f\"{Y.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = int(len(X) * 0.8)\n",
    "n2 = int(len(X) * 0.9)\n",
    "print(\"Split Global Dataset on lines:\", n1, n2)\n",
    "X_tr = X[:n1]\n",
    "Y_tr = Y[:n1]\n",
    "X_val = X[n1:n2]\n",
    "Y_val = Y[n1:n2]\n",
    "X_tst = X[n2:]\n",
    "Y_tst = Y[n2:]\n",
    "print(f\"{X_tr.shape=}\")\n",
    "print(f\"{X_val.shape=}\")\n",
    "print(f\"{X_tst.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(5/3) * (hidden**-0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.manual_seed(42)\n",
    "E_w = torch.randn(size=(voc_size, emb), device=device) * ((emb*att)**-0.5) * (5/3)\n",
    "H0_w = torch.randn(size=(emb*att, hidden), device=device) * (hidden**-0.5) * (5/3)\n",
    "H0_b = torch.randn(size=(hidden,), device=device) * 0.01\n",
    "H1_w = torch.randn(size=(hidden, voc_size), device=device) * (voc_size)**-0.5\n",
    "H1_b = torch.randn(size=(voc_size,), device=device) * 0.01\n",
    "params = [E_w, H0_w, H0_b, H1_w, H1_b]\n",
    "nparams = sum([t.numel() for t in params])\n",
    "print(f\"{nparams=}\")\n",
    "for t in params:\n",
    "    t.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X):\n",
    "    Ey = E_w[X].flatten(1, 2)\n",
    "    H0_y = torch.tanh(Ey @ H0_w + H0_b)\n",
    "    L = H0_y @ H1_w + H1_b\n",
    "    return L\n",
    "\n",
    "def calc_loss(L, Y):\n",
    "    return F.cross_entropy(L, Y)\n",
    "\n",
    "def backward(loss: torch.Tensor):\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            p.grad.zero_()\n",
    "    loss.backward()\n",
    "\n",
    "def update_params(step):\n",
    "    for p in params:\n",
    "        p.data -= step * p.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X0, Y0, n):\n",
    "    rids = torch.randint(0, n1, (n,), device=device)\n",
    "    return X0[rids], Y0[rids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LL=[]\n",
    "\n",
    "batch = 32\n",
    "X0, Y0 = get_batch(X_tr, Y_tr, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "L = forward(X0)\n",
    "loss = calc_loss(L, Y0)\n",
    "print(\"No Training. loss:\", loss.item())\n",
    "\n",
    "# initial training\n",
    "for i in range(1):\n",
    "    backward(loss)\n",
    "    update_params(0.1)\n",
    "    X0, Y0 = get_batch(X_tr, Y_tr, batch)\n",
    "    L = forward(X0)\n",
    "    loss = calc_loss(L, Y0)\n",
    "print(\"After Initial training. loss: \", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dead neurons:\", torch.where(torch.abs(H0_w.grad)<0.0001, 1,0).sum())\n",
    "with torch.no_grad():\n",
    "    X0, Y0 = X_val, Y_val\n",
    "    Ey = E_w[X0].flatten(1, 2)\n",
    "    H0_y = torch.tanh(Ey @ H0_w + H0_b)\n",
    "    L = H0_y @ H1_w + H1_b\n",
    "    gg = L.flatten().detach().cpu().numpy()\n",
    "    print(gg.shape)\n",
    "    plt.hist(gg, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N = 10000\n",
    "steps=[0.1, 0.03, 0.01, 0.003, 0.0001]\n",
    "#steps=[0.1]\n",
    "#steps=[0.01]\n",
    "#steps=[0.03]\n",
    "#steps=[0.0001]\n",
    "WIN = []\n",
    "for step in steps:\n",
    "    for i in range(N):\n",
    "        backward(loss)\n",
    "        update_params(step)\n",
    "        X0, Y0 = get_batch(X_tr, Y_tr, batch)\n",
    "        L = forward(X0)\n",
    "        loss = calc_loss(L, Y0)\n",
    "        WIN.append(loss.item())\n",
    "        if i % 200 == 0:\n",
    "          LL.append(np.mean(WIN))\n",
    "          WIN = []\n",
    "        \n",
    "    print(f\"Step {step} done. Loss: {LL[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation / Test\n",
    "with torch.no_grad():\n",
    "    L = forward(X_val)\n",
    "    loss = calc_loss(L, Y_val)\n",
    "    print(\"Validation loss\", loss)\n",
    "\n",
    "    L = forward(X_tst)\n",
    "    loss = calc_loss(L, Y_tst)\n",
    "    print(\"Test loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "df = pd.DataFrame(LL, columns=['X'])\n",
    "X_col=df['X']\n",
    "MA_X_col = df['X'].rolling(window=20).mean()\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(X_col, 'b-', label='loss')\n",
    "plt.plot(MA_X_col, 'r-', label='MA20')\n",
    "plt.grid(linestyle='--')\n",
    "plt.legend(loc='upper center')\n",
    "plt.show()\n",
    "print(\"Tail of Moving Average column\")\n",
    "MA_X_col[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=encode(\"emm\")\n",
    "\n",
    "for i in range(100):\n",
    "    x = torch.tensor([s[-att:]], device=device)\n",
    "    L = forward(x)\n",
    "    L = torch.softmax(L, dim=-1)\n",
    "    ci = torch.multinomial(L, num_samples=1).item()\n",
    "    #ci = int(torch.argmax(L).item())\n",
    "    c = itos[ci]\n",
    "    if ci == 0:\n",
    "        break\n",
    "    s += [ci]\n",
    "print(decode(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beg = []\n",
    "batch = words[30:55]\n",
    "for w in batch:\n",
    "    if len(w) < att:\n",
    "        w = \".\" * (len(w) - att) + w\n",
    "    beg.append(encode(w[:att]))\n",
    "x = torch.tensor(beg, device=device)\n",
    "#print(x)\n",
    "for i in range(7):\n",
    "    L = forward(x[:,-att:])\n",
    "    L = torch.softmax(L, dim=-1)\n",
    "    y = torch.multinomial(L, num_samples=1)\n",
    "    #y = torch.argmax(L, dim=1, keepdim=True)\n",
    "    x = torch.cat([x,y],dim=-1)\n",
    "for i, row in enumerate(x.detach().cpu().numpy()):\n",
    "    print(decode(row, True), \"   \", batch[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_emb():\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(E_w[:,0].detach().cpu().numpy(), E_w[:,1].detach().cpu().numpy(), s=200)\n",
    "    for i in range(voc_size):\n",
    "        plt.text(E_w[i,0].item(), E_w[i,1].item(), itos[i], ha=\"center\", va=\"center\", color=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_emb()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
