{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict name based on first several letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "print(f\"{torch.cuda.is_available()=}\")\n",
    "device=\"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device=\"cuda:0\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "itos = {0:'.'}\n",
    "for i, c in enumerate(string.ascii_lowercase):\n",
    "    itos[i+1]=c  \n",
    "stoi = {s:i for i, s in itos.items()}\n",
    "print(stoi)\n",
    "voc_size=len(itos)\n",
    "print(f\"{voc_size=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(ss):\n",
    "    res = [stoi[c] for c in ss]\n",
    "    return res\n",
    "\n",
    "def decode(ii, tilldot=False):\n",
    "    ch = False\n",
    "    res = []\n",
    "    for i in ii:\n",
    "        if i == 0 and tilldot and ch:\n",
    "            break\n",
    "        if not(ch) and i != 0:\n",
    "            ch = True\n",
    "        res.append(itos[i])\n",
    "    return ''.join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_f = \"names.txt\"\n",
    "with open(names_f) as f:\n",
    "    words = f.read().splitlines()\n",
    "\n",
    "#random.seed(42)\n",
    "random.shuffle(words)\n",
    "print(words[:3])\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_word(w, bsz, X, Y):\n",
    "    x = \".\"*bsz\n",
    "    xi = [0]*bsz\n",
    "    for y in w:\n",
    "        yi = stoi[y]\n",
    "        X.append(xi)\n",
    "        Y.append(yi)\n",
    "        xi = xi[1:]\n",
    "        xi.append(yi)\n",
    "    X.append(xi)\n",
    "    Y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att=3\n",
    "emb=10\n",
    "hidden = 200\n",
    "\n",
    "Xa, Ya = [], []\n",
    "for w in words:\n",
    "    add_word(w, att, Xa, Ya)\n",
    "X = torch.tensor(Xa, device=device)\n",
    "Y = torch.tensor(Ya, device=device)\n",
    "print(f\"{X.shape=}\")\n",
    "print(f\"{Y.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = int(len(X) * 0.8)\n",
    "n2 = int(len(X) * 0.9)\n",
    "print(\"Split Global Dataset on lines:\", n1, n2)\n",
    "X_tr = X[:n1]\n",
    "Y_tr = Y[:n1]\n",
    "X_val = X[n1:n2]\n",
    "Y_val = Y[n1:n2]\n",
    "X_tst = X[n2:]\n",
    "Y_tst = Y[n2:]\n",
    "print(f\"{X_tr.shape=}\")\n",
    "print(f\"{X_val.shape=}\")\n",
    "print(f\"{X_tst.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, in_f, out_f, bias=True, device=None, dtype=None) -> None:\n",
    "        self.weight = torch.randn(size=(in_f, out_f), dtype=dtype, device=device) * (in_f**-0.5)\n",
    "        self.bias = torch.zeros(out_f, dtype=dtype, device=device) if bias else None\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        if self.bias is not None:\n",
    "            return [self.weight, self.bias]\n",
    "        return [self.weight]\n",
    "    \n",
    "class Tanh:\n",
    "    def __call__(self, x) -> Any:\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "class BatchNorm1d:\n",
    "    def __init__(self, num_f, eps=1e-05, momentum=0.1, device=None, dtype=None) -> None:\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        self.gamma = torch.ones(num_f, dtype=dtype, device=device)\n",
    "        self.beta = torch.zeros(num_f, dtype=dtype, device=device)\n",
    "        self.running_mean = torch.zeros(num_f, dtype=dtype, device=device)\n",
    "        self.running_var = torch.ones(num_f, dtype=dtype, device=device)\n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            xmean = x.mean(0, keepdim=True) # batch mean\n",
    "            xvar = x.var(0, keepdim=True) # batch var\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.manual_seed(42)\n",
    "E_w = torch.randn(size=(voc_size, emb), device=device)\n",
    "\n",
    "layers = [\n",
    "    Linear(emb*att, hidden, device=device), BatchNorm1d(hidden, device=device), Tanh(),\n",
    "    Linear(hidden, voc_size, device=device)\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    layers[-1].weight *= 0.1 # reduce last layer confidence\n",
    "\n",
    "params = [E_w] + [p for l in layers for p in l.parameters()]\n",
    "nparams = sum([t.numel() for t in params])\n",
    "print(f\"{nparams=}\")\n",
    "for t in params:\n",
    "    t.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    y = E_w[x].flatten(1)\n",
    "    for l in layers:\n",
    "        y = l(y)\n",
    "    return y\n",
    "\n",
    "def calc_loss(L, Y):\n",
    "    return F.cross_entropy(L, Y)\n",
    "\n",
    "def backward(loss: torch.Tensor):\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            p.grad.zero_()\n",
    "    loss.backward()\n",
    "\n",
    "def update_params(step):\n",
    "    for p in params:\n",
    "        p.data -= step * p.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X0, Y0, n):\n",
    "    rids = torch.randint(0, n1, (n,), device=device)\n",
    "    return X0[rids], Y0[rids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LL=[]\n",
    "\n",
    "batch = 32\n",
    "X0, Y0 = get_batch(X_tr, Y_tr, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "L = forward(X0)\n",
    "loss = calc_loss(L, Y0)\n",
    "print(\"No Training. loss:\", loss.item())\n",
    "\n",
    "# initial training\n",
    "for i in range(1):\n",
    "    backward(loss)\n",
    "    update_params(0.1)\n",
    "    X0, Y0 = get_batch(X_tr, Y_tr, batch)\n",
    "    L = forward(X0)\n",
    "    loss = calc_loss(L, Y0)\n",
    "print(\"After Initial training. loss: \", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"dead neurons:\", torch.where(torch.abs(H0_w.grad)<0.0001, 1,0).sum())\n",
    "# with torch.no_grad():\n",
    "#     X0, Y0 = X_val, Y_val\n",
    "#     Ey = E_w[X0].flatten(1)\n",
    "#     H0_y = torch.tanh(Ey @ H0_w + H0_b)\n",
    "#     L = H0_y @ H1_w + H1_b\n",
    "#     gg = H0_y.flatten().detach().cpu().numpy()\n",
    "#     print(gg.shape)\n",
    "#     plt.hist(gg, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in layers:\n",
    "    l.training = True\n",
    "\n",
    "N = 20000\n",
    "#steps=[0.01, 0.003]\n",
    "steps=[0.1, 0.03, 0.01, 0.003, 0.0001]\n",
    "#steps=[0.1]\n",
    "#steps=[0.01]\n",
    "#steps=[0.03]\n",
    "#steps=[0.0001]\n",
    "\n",
    "X0, Y0 = get_batch(X_tr, Y_tr, batch)\n",
    "L = forward(X0)\n",
    "loss = calc_loss(L, Y0)\n",
    "\n",
    "WIN = []\n",
    "for step in steps:\n",
    "    for i in range(N):\n",
    "        backward(loss)\n",
    "        update_params(step)\n",
    "        X0, Y0 = get_batch(X_tr, Y_tr, batch)\n",
    "        L = forward(X0)\n",
    "        loss = calc_loss(L, Y0)\n",
    "        WIN.append(loss.item())\n",
    "        if i % 200 == 0:\n",
    "          LL.append(np.mean(WIN))\n",
    "          WIN = []\n",
    "        \n",
    "    print(f\"Step {step} done. Loss: {LL[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation / Test\n",
    "for l in layers:\n",
    "    l.training = False\n",
    "\n",
    "with torch.no_grad():\n",
    "    L = forward(X_val)\n",
    "    loss = calc_loss(L, Y_val)\n",
    "    print(\"Validation loss\", loss)\n",
    "\n",
    "    L = forward(X_tst)\n",
    "    loss = calc_loss(L, Y_tst)\n",
    "    print(\"Test loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "df = pd.DataFrame(LL, columns=['X'])\n",
    "X_col=df['X']\n",
    "MA_X_col = df['X'].rolling(window=20).mean()\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(X_col, 'b-', label='loss')\n",
    "plt.plot(MA_X_col, 'r-', label='MA20')\n",
    "plt.grid(linestyle='--')\n",
    "plt.legend(loc='upper center')\n",
    "plt.show()\n",
    "print(\"Tail of Moving Average column\")\n",
    "MA_X_col[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=encode(\"emm\")\n",
    "\n",
    "for i in range(100):\n",
    "    x = torch.tensor([s[-att:]], device=device)\n",
    "    L = forward(x)\n",
    "    L = torch.softmax(L, dim=-1)\n",
    "    ci = torch.multinomial(L, num_samples=1).item()\n",
    "    #ci = int(torch.argmax(L).item())\n",
    "    c = itos[ci]\n",
    "    if ci == 0:\n",
    "        break\n",
    "    s += [ci]\n",
    "print(decode(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beg = []\n",
    "batch = words[30:55]\n",
    "for w in batch:\n",
    "    if len(w) < att:\n",
    "        w = \".\" * (len(w) - att) + w\n",
    "    beg.append(encode(w[:att]))\n",
    "x = torch.tensor(beg, device=device)\n",
    "#print(x)\n",
    "for i in range(7):\n",
    "    L = forward(x[:,-att:])\n",
    "    L = torch.softmax(L, dim=-1)\n",
    "    y = torch.multinomial(L, num_samples=1)\n",
    "    #y = torch.argmax(L, dim=1, keepdim=True)\n",
    "    x = torch.cat([x,y],dim=-1)\n",
    "for i, row in enumerate(x.detach().cpu().numpy()):\n",
    "    print(decode(row, True), \"   \", batch[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_emb():\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(E_w[:,4].detach().cpu().numpy(), E_w[:,5].detach().cpu().numpy(), s=200)\n",
    "    for i in range(voc_size):\n",
    "        plt.text(E_w[i,4].item(), E_w[i,5].item(), itos[i], ha=\"center\", va=\"center\", color=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_emb()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
